# -*- coding: utf-8 -*-
# @Author: Haozhe Xie
# @Date:   2019-08-30 10:01:53
# @Last Modified by:   Haozhe Xie
# @Last Modified time: 2019-09-05 11:40:59
# @Email:  cshzxie@gmail.com

import numpy as np
import sys
import torch
import unittest

sys.path.append('../')
import permutohedral_layer


def get_filter_size(neighborhood_size, in_channels):
    return (neighborhood_size + 1)**(in_channels + 1) - neighborhood_size**(in_channels + 1)


class PermutohedralLayerTest(unittest.TestCase):
    def setUp(self):
        self.size1 = {'BATCH_SIZE': 1, 'N_DATA_CHANNELS': 2, 'N_IN_CHANNELS': 2, 'HEIGHT': 5, 'WIDTH': 6}
        self.data1 = torch.from_numpy(
            np.array([
                0, 0, 1.56894, 1.71588, 0, 0, 1.56518, 1.53057, 0.76686, -0.592583, 0.534071, -1.02683, -0.604775,
                1.68742, -0.89592, -1.50184, 0.506595, 0.135528, -0.48416, 0.412273, 0.503575, 0.507338, 0.456594,
                1.02362, 0, 0, 0.929329, 0.4986, 0, 0, 0, 0, -0.642905, 0.437683, 0, 0, 0.542442, -1.87837, 0.816111,
                0.461546, 0.353122, 1.43353, -0.519231, 0.114483, -0.439219, -0.238896, 0.309394, -1.6441, -0.875012,
                0.186592, -0.92626, 0.243145, -0.0358702, 1.23521, 0, 0, 0.585628, -0.843697, 0, 0
            ]).reshape(self.size1['BATCH_SIZE'], self.size1['N_DATA_CHANNELS'], self.size1['HEIGHT'],
                       self.size1['WIDTH']).astype(np.float32))
        self.features1 = torch.from_numpy(
            np.array([
                0, 0, 0, 0, 0, 0, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.6, 0.6, 0.6, 0.6, 0.6,
                0.6, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0, 0.2, 0.4, 0.6, 0.8, 1, 0, 0.2, 0.4, 0.6, 0.8, 1, 0, 0.2, 0.4,
                0.6, 0.8, 1, 0, 0.2, 0.4, 0.6, 0.8, 1, 0, 0.2, 0.4, 0.6, 0.8, 1
            ]).reshape(self.size1['BATCH_SIZE'], self.size1['N_IN_CHANNELS'], self.size1['HEIGHT'],
                       self.size1['WIDTH']).astype(np.float32))

        self.size2 = {'BATCH_SIZE': 2, 'N_DATA_CHANNELS': 3, 'N_IN_CHANNELS': 2, 'HEIGHT': 5, 'WIDTH': 6}
        self.data2 = torch.from_numpy(
            np.array([
                0.2297, 0.9031, 0.5464, 0.2351, 0.6571, 0.5585, 0.3225, 0.8402, 0.2439, 0.3138, 0.6233, 0.6573, 0.4003,
                0.4778, 0.7889, 0.4275, 0.1488, 0.4584, 0.4715, 0.8742, 0.6209, 0.4051, 0.8950, 0.9467, 0.3952, 0.0575,
                0.0405, 0.2303, 0.2941, 0.6675, 0.2729, 0.7331, 0.6030, 0.1424, 0.1135, 0.2974, 0.0139, 0.4575, 0.4795,
                0.5677, 0.4533, 0.0499, 0.9274, 0.8068, 0.3829, 0.6033, 0.2450, 0.4436, 0.7308, 0.9591, 0.5644, 0.1489,
                0.0536, 0.8247, 0.1526, 0.7276, 0.2556, 0.0656, 0.7934, 0.0850, 0.3384, 0.3505, 0.9437, 0.7107, 0.4308,
                0.8470, 0.9034, 0.3055, 0.3580, 0.6545, 0.1472, 0.5787, 0.8630, 0.9750, 0.0853, 0.0977, 0.4031, 0.8193,
                0.6598, 0.6552, 0.9340, 0.4207, 0.9220, 0.6315, 0.4577, 0.4308, 0.4022, 0.9093, 0.9483, 0.7600, 0.5738,
                0.3492, 0.8163, 0.1479, 0.7706, 0.1126, 0.3089, 0.6661, 0.9951, 0.7380, 0.9936, 0.7604, 0.3365, 0.3227,
                0.6734, 0.7063, 0.2860, 0.6588, 0.7504, 0.2900, 0.9391, 0.0706, 0.9484, 0.5763, 0.8933, 0.5264, 0.5589,
                0.3317, 0.9958, 0.6312, 0.8711, 0.1608, 0.9729, 0.9458, 0.4347, 0.3480, 0.4616, 0.5678, 0.9693, 0.0690,
                0.9508, 0.4754, 0.7044, 0.0779, 0.5696, 0.7265, 0.2347, 0.8719, 0.0363, 0.7187, 0.1260, 0.2687, 0.3113,
                0.3489, 0.2698, 0.5030, 0.3641, 0.5549, 0.0807, 0.4281, 0.8064, 0.8014, 0.7823, 0.7218, 0.5811, 0.7904,
                0.9201, 0.2437, 0.4996, 0.3926, 0.4099, 0.4092, 0.0189, 0.3186, 0.6671, 0.6964, 0.7090, 0.5101, 0.8552,
                0.6436, 0.2458, 0.6551, 0.6447, 0.4031, 0.8646, 0.6621, 0.0077, 0.0136, 0.5518, 0.9493
            ]).reshape(self.size2['BATCH_SIZE'], self.size2['N_DATA_CHANNELS'], self.size2['HEIGHT'],
                       self.size2['WIDTH']).astype(np.float32))
        self.features2 = torch.from_numpy(
            np.array([
                0.2908, 0.5607, 0.3978, 0.8210, 0.4916, 0.9957, 0.8447, 0.2887, 0.0643, 0.8021, 0.4798, 0.2987, 0.8123,
                0.2579, 0.7208, 0.3777, 0.2443, 0.2797, 0.1462, 0.3429, 0.2094, 0.5826, 0.6923, 0.9023, 0.6700, 0.6155,
                0.5538, 0.8558, 0.0822, 0.3630, 0.9609, 0.2412, 0.2415, 0.8213, 0.2975, 0.7486, 0.9777, 0.8239, 0.7252,
                0.7416, 0.0214, 0.2048, 0.8637, 0.2563, 0.4480, 0.5416, 0.0479, 0.0229, 0.7677, 0.7913, 0.8414, 0.9984,
                0.5061, 0.8683, 0.8092, 0.4373, 0.7967, 0.1083, 0.4352, 0.9260, 0.7869, 0.6629, 0.8766, 0.8622, 0.4202,
                0.3656, 0.3297, 0.7473, 0.9442, 0.1292, 0.6412, 0.8646, 0.1709, 0.7465, 0.9541, 0.8721, 0.1030, 0.9609,
                0.3918, 0.6812, 0.1145, 0.5206, 0.2231, 0.4997, 0.5820, 0.8335, 0.0037, 0.1931, 0.0012, 0.3974, 0.2565,
                0.5534, 0.8235, 0.6348, 0.5502, 0.0419, 0.8115, 0.3664, 0.7118, 0.9897, 0.3726, 0.1009, 0.5420, 0.8868,
                0.0645, 0.2936, 0.5070, 0.6676, 0.5510, 0.4801, 0.1589, 0.7849, 0.2240, 0.4856, 0.4101, 0.8115, 0.8501,
                0.9783, 0.3405, 0.6317
            ]).reshape(self.size2['BATCH_SIZE'], self.size2['N_IN_CHANNELS'], self.size2['HEIGHT'],
                       self.size2['WIDTH']).astype(np.float32))

    def test_forward_without_bias_1(self):
        NEIGHBORHOOD_SIZE = 3
        N_OUTPUT_CHANNELS = 1
        GROUP = 1
        DO_SKIP_BLUR = False
        # USE_BIAS_TERM = False

        weights = torch.from_numpy(
            np.array([
                0.1323, 0.6974, 0.0385, 0.1482, 0.9143, 0.3580, 0.7855, 0.6853, 0.7724, 0.8354, 0.5875, 0.0648, 0.2228,
                0.0221, 0.1667, 0.3385, 0.5716, 0.8733, 0.7973, 0.6162, 0.8735, 0.4067, 0.4518, 0.5935, 0.1076, 0.4054,
                0.7348, 0.6255, 0.6997, 0.1674, 0.6014, 0.1302, 0.2214, 0.1487, 0.9410, 0.7894, 0.3460, 0.1162, 0.4337,
                0.6263, 0.8687, 0.3424, 0.3681, 0.9897, 0.1241, 0.3494, 0.5519, 0.0626, 0.4717, 0.1418, 0.0482, 0.4744,
                0.4467, 0.5200, 0.8097, 0.0078, 0.0759, 0.0823, 0.4734, 0.3025, 0.4921, 0.2631, 0.3241, 0.3063, 0.0510,
                0.0217, 0.4484, 0.5637, 0.1158, 0.9030, 0.3043, 0.4858, 0.0309, 0.9056
            ]).reshape(N_OUTPUT_CHANNELS, self.size1['N_DATA_CHANNELS'] // GROUP, 1,
                       get_filter_size(NEIGHBORHOOD_SIZE, self.size1['N_IN_CHANNELS'])).astype(np.float32)).cuda()
        bias = torch.Tensor().cuda()
        bias_multiplier = torch.Tensor().cuda()

        expected_output = [
            0.141078, 0.160562, 0.179919, 0.199151, 0.218258, 0.237243, 0.157174, 0.149898, 0.16911, 0.1882, 0.207166,
            0.232033, 0.17601, 0.146237, 0.1585, 0.177448, 0.197525, 0.226493, 0.198352, 0.162737, 0.148082, 0.166892,
            0.191128, 0.220592, 0.225278, 0.182046, 0.151396, 0.158187, 0.184329, 0.214291
        ]
        actual_output = permutohedral_layer.PermutohedralFunction.apply(NEIGHBORHOOD_SIZE, GROUP, N_OUTPUT_CHANNELS,
                                                                        DO_SKIP_BLUR, weights, bias, bias_multiplier,
                                                                        self.data1.cuda(),
                                                                        self.features1.cuda()).view(-1)
        for i in range(len(expected_output)):
            assert abs(expected_output[i] - actual_output[i].item()) < 1e-6

    def test_forward_without_bias_2(self):
        NEIGHBORHOOD_SIZE = 3
        N_OUTPUT_CHANNELS = 2
        GROUP = 1
        DO_SKIP_BLUR = False
        # USE_BIAS_TERM = False

        weights = torch.from_numpy(
            np.array([
                0.5021, 0.0808, 0.1300, 0.0037, 0.0900, 0.7380, 0.5003, 0.5992, 0.8590, 0.5942, 0.2779, 0.4272, 0.2538,
                0.0785, 0.8325, 0.8073, 0.2169, 0.6298, 0.0537, 0.7440, 0.0398, 0.2114, 0.0280, 0.6852, 0.8143, 0.3317,
                0.9143, 0.9252, 0.0743, 0.1257, 0.0342, 0.4272, 0.1521, 0.8283, 0.4482, 0.0655, 0.0039, 0.0522, 0.1913,
                0.3830, 0.4114, 0.8568, 0.7087, 0.1507, 0.6350, 0.1647, 0.7555, 0.1491, 0.6394, 0.0879, 0.0163, 0.0635,
                0.5957, 0.8960, 0.9698, 0.7643, 0.5854, 0.3660, 0.0755, 0.7076, 0.0769, 0.8268, 0.9251, 0.2357, 0.4510,
                0.8530, 0.3781, 0.4296, 0.4140, 0.2051, 0.3979, 0.5243, 0.0386, 0.4116, 0.3210, 0.1167, 0.4434, 0.6002,
                0.9113, 0.4777, 0.6002, 0.1779, 0.7167, 0.5999, 0.5278, 0.8685, 0.7545, 0.6099, 0.6418, 0.4805, 0.7603,
                0.3483, 0.9601, 0.8955, 0.0394, 0.5211, 0.8051, 0.1423, 0.9552, 0.3077, 0.3644, 0.7126, 0.8151, 0.4037,
                0.9388, 0.6032, 0.3575, 0.4161, 0.3829, 0.6212, 0.3134, 0.0478, 0.6631, 0.6889, 0.0525, 0.6416, 0.1704,
                0.4762, 0.2783, 0.2961, 0.6712, 0.5909, 0.9193, 0.4749, 0.7097, 0.3195, 0.0790, 0.5283, 0.1373, 0.8415,
                0.6499, 0.7637, 0.8716, 0.0146, 0.8270, 0.0501, 0.7787, 0.5214, 0.3531, 0.1292, 0.0786, 0.4242, 0.0910,
                0.3798, 0.4713, 0.0436, 0.1656, 0.1864
            ]).reshape(N_OUTPUT_CHANNELS, self.size1['N_DATA_CHANNELS'] // GROUP, 1,
                       get_filter_size(NEIGHBORHOOD_SIZE, self.size1['N_IN_CHANNELS'])).astype(np.float32)).cuda()
        bias = torch.Tensor().cuda()
        bias_multiplier = torch.Tensor().cuda()

        expected_output = [
            0.103097, 0.100565, 0.0980489, 0.0955495, 0.0930663, 0.090599, 0.109696, 0.130067, 0.12748, 0.124909,
            0.122354, 0.12228, 0.117418, 0.153899, 0.156371, 0.153732, 0.152072, 0.155967, 0.126578, 0.164483,
            0.184737, 0.182032, 0.185473, 0.191856, 0.137617, 0.176868, 0.204695, 0.212958, 0.220979, 0.230172,
            0.192484, 0.165642, 0.138974, 0.112479, 0.0861561, 0.0600023, 0.207423, 0.187871, 0.161378, 0.135056,
            0.108903, 0.0894282, 0.224906, 0.207818, 0.183372, 0.15722, 0.133422, 0.120717, 0.245642, 0.223959,
            0.204966, 0.178983, 0.164944, 0.154052, 0.270632, 0.242848, 0.22315, 0.206141, 0.198452, 0.18964
        ]
        actual_output = permutohedral_layer.PermutohedralFunction.apply(NEIGHBORHOOD_SIZE, GROUP, N_OUTPUT_CHANNELS,
                                                                        DO_SKIP_BLUR, weights, bias, bias_multiplier,
                                                                        self.data1.cuda(),
                                                                        self.features1.cuda()).view(-1)
        for i in range(len(expected_output)):
            assert abs(expected_output[i] - actual_output[i].item()) < 1e-6

    def test_forward_without_bias_3(self):
        NEIGHBORHOOD_SIZE = 3
        N_OUTPUT_CHANNELS = 2
        GROUP = 2
        DO_SKIP_BLUR = False
        # USE_BIAS_TERM = False

        weights = torch.from_numpy(
            np.array([
                0.9563, 0.9710, 0.1391, 0.7237, 0.0513, 0.9796, 0.5293, 0.8683, 0.9887, 0.3814, 0.5046, 0.0728, 0.6375,
                0.8009, 0.7446, 0.4081, 0.4133, 0.7653, 0.7872, 0.3806, 0.9470, 0.4092, 0.7763, 0.7680, 0.5838, 0.5031,
                0.2360, 0.6857, 0.0998, 0.0508, 0.3512, 0.4108, 0.2213, 0.9582, 0.9080, 0.8887, 0.1852, 0.5220, 0.4328,
                0.5508, 0.1733, 0.1334, 0.0827, 0.5330, 0.1629, 0.1145, 0.4290, 0.6580, 0.9343, 0.4470, 0.0170, 0.6574,
                0.7234, 0.7337, 0.4713, 0.2950, 0.3095, 0.9189, 0.6696, 0.4937, 0.1802, 0.8292, 0.5326, 0.3017, 0.9982,
                0.8356, 0.4158, 0.2481, 0.3167, 0.0646, 0.3026, 0.0480, 0.5483, 0.4559
            ]).reshape(N_OUTPUT_CHANNELS, self.size1['N_DATA_CHANNELS'] // GROUP, 1,
                       get_filter_size(NEIGHBORHOOD_SIZE, self.size1['N_IN_CHANNELS'])).astype(np.float32)).cuda()
        bias = torch.Tensor().cuda()
        bias_multiplier = torch.Tensor().cuda()

        expected_output = [
            0.339508, 0.350793, 0.362005, 0.373145, 0.384212, 0.395208, 0.327138, 0.343477, 0.354609, 0.365669,
            0.376659, 0.395246, 0.312663, 0.335108, 0.347348, 0.35833, 0.371036, 0.395287, 0.295495, 0.322394,
            0.340219, 0.351124, 0.370336, 0.39533, 0.274803, 0.307516, 0.330708, 0.347095, 0.369591, 0.395376,
            -0.0264046, -0.0252216, -0.0240462, -0.0228785, -0.0217183, -0.0205656, -0.0275501, -0.0262135, -0.0250458,
            -0.0238856, -0.0227329, -0.0260976, -0.0288906, -0.0272228, -0.026027, -0.0248743, -0.0249181, -0.0319799,
            -0.0304805, -0.0284323, -0.0269904, -0.0258451, -0.0305557, -0.0382468, -0.0323967, -0.0298478, -0.0280408,
            -0.0292291, -0.0365484, -0.0449373
        ]
        actual_output = permutohedral_layer.PermutohedralFunction.apply(NEIGHBORHOOD_SIZE, GROUP, N_OUTPUT_CHANNELS,
                                                                        DO_SKIP_BLUR, weights, bias, bias_multiplier,
                                                                        self.data1.cuda(),
                                                                        self.features1.cuda()).view(-1)
        for i in range(len(expected_output)):
            assert abs(expected_output[i] - actual_output[i].item()) < 1e-6

    def test_forward_with_bias1(self):
        NEIGHBORHOOD_SIZE = 3
        N_OUTPUT_CHANNELS = 2
        GROUP = 2
        DO_SKIP_BLUR = False
        # USE_BIAS_TERM = True

        weights = torch.from_numpy(
            np.array([
                0.9563, 0.9710, 0.1391, 0.7237, 0.0513, 0.9796, 0.5293, 0.8683, 0.9887, 0.3814, 0.5046, 0.0728, 0.6375,
                0.8009, 0.7446, 0.4081, 0.4133, 0.7653, 0.7872, 0.3806, 0.9470, 0.4092, 0.7763, 0.7680, 0.5838, 0.5031,
                0.2360, 0.6857, 0.0998, 0.0508, 0.3512, 0.4108, 0.2213, 0.9582, 0.9080, 0.8887, 0.1852, 0.5220, 0.4328,
                0.5508, 0.1733, 0.1334, 0.0827, 0.5330, 0.1629, 0.1145, 0.4290, 0.6580, 0.9343, 0.4470, 0.0170, 0.6574,
                0.7234, 0.7337, 0.4713, 0.2950, 0.3095, 0.9189, 0.6696, 0.4937, 0.1802, 0.8292, 0.5326, 0.3017, 0.9982,
                0.8356, 0.4158, 0.2481, 0.3167, 0.0646, 0.3026, 0.0480, 0.5483, 0.4559
            ]).reshape(N_OUTPUT_CHANNELS, self.size1['N_DATA_CHANNELS'] // GROUP, 1,
                       get_filter_size(NEIGHBORHOOD_SIZE, self.size1['N_IN_CHANNELS'])).astype(np.float32)).cuda()
        bias = torch.from_numpy(np.array([0.0818, 0.8894]).reshape(1, 1, 1,
                                                                   N_OUTPUT_CHANNELS).astype(np.float32)).cuda()
        bias_multiplier = torch.from_numpy(
            np.array([
                0.4784, 0.9353, 0.7067, 0.7665, 0.6245, 0.8526, 0.0751, 0.7218, 0.8618, 0.1907, 0.0649, 0.9583, 0.6774,
                0.1310, 0.1830, 0.2861, 0.7582, 0.1888, 0.3619, 0.5874, 0.5918, 0.9045, 0.4876, 0.7668, 0.9014, 0.7652,
                0.8306, 0.0115, 0.3857, 0.2601
            ]).reshape(1, 1, 1, self.size1['HEIGHT'] * self.size1['WIDTH']).astype(np.float32)).cuda()

        expected_output = [
            0.378641, 0.427301, 0.419813, 0.435844, 0.435296, 0.464951, 0.333282, 0.40252, 0.425104, 0.381268,
            0.381968, 0.473635, 0.368075, 0.345824, 0.362317, 0.381733, 0.433057, 0.41073, 0.325098, 0.370444,
            0.388628, 0.425112, 0.410221, 0.458054, 0.348538, 0.37011, 0.398652, 0.348035, 0.401142, 0.416652,
            0.399084, 0.806634, 0.604493, 0.658847, 0.533712, 0.737737, 0.0392438, 0.615755, 0.741439, 0.145723,
            0.0349892, 0.826214, 0.573589, 0.0892886, 0.136733, 0.229583, 0.649425, 0.135939, 0.291393, 0.494001,
            0.499357, 0.778617, 0.403116, 0.643745, 0.769309, 0.650721, 0.710695, -0.019001, 0.306493, 0.186396
        ]
        actual_output = permutohedral_layer.PermutohedralFunction.apply(NEIGHBORHOOD_SIZE, GROUP, N_OUTPUT_CHANNELS,
                                                                        DO_SKIP_BLUR, weights, bias, bias_multiplier,
                                                                        self.data1.cuda(),
                                                                        self.features1.cuda()).view(-1)
        for i in range(len(expected_output)):
            assert abs(expected_output[i] - actual_output[i].item()) < 1e-6

    def test_forward_with_bias2(self):
        NEIGHBORHOOD_SIZE = 3
        N_OUTPUT_CHANNELS = 3
        GROUP = 2
        DO_SKIP_BLUR = False
        # USE_BIAS_TERM = True

        weights = torch.from_numpy(
            np.array([
                0.56018, 0.66077, 0.47967, 0.93142, 0.77315, 0.21726, 0.33502, 0.40209, 0.26237, 0.53436, 0.96326,
                0.77337, 0.094630, 0.50813, 0.0063933, 0.67345, 0.10891, 0.43981, 0.99030, 0.41358, 0.069789, 0.24911,
                0.69059, 0.30854, 0.027439, 0.82271, 0.33332, 0.098550, 0.84825, 0.79608, 0.43093, 0.68114, 0.17651,
                0.69392, 0.96957, 0.19845, 0.72236, 0.52883, 0.82716, 0.53459, 0.80694, 0.0017458, 0.26308, 0.99736,
                0.63688, 0.044737, 0.64004, 0.34618, 0.039204, 0.75342, 0.22527, 0.40271, 0.86862, 0.23436, 0.00039381,
                0.79662, 0.50522, 0.97183, 0.95994, 0.12843, 0.17314, 0.049051, 0.32262, 0.084470, 0.17357, 0.76326,
                0.019772, 0.49854, 0.21618, 0.14949, 0.28325, 0.77910, 0.15810, 0.17874, 0.75375, 0.43863, 0.42561,
                0.52256, 0.94468, 0.00015473, 0.55581, 0.085251, 0.87456, 0.032248, 0.49668, 0.19325, 0.31288, 0.98346,
                0.93764, 0.17975, 0.55441, 0.28111, 0.22934, 0.56666, 0.90331, 0.95067, 0.76162, 0.92267, 0.21433,
                0.85191, 0.71548, 0.074870, 0.69556, 0.81133, 0.69694, 0.91801, 0.56038, 0.92392, 0.68421, 0.065922,
                0.56490
            ]).reshape(N_OUTPUT_CHANNELS, self.size2['N_DATA_CHANNELS'] // GROUP, 1,
                       get_filter_size(NEIGHBORHOOD_SIZE, self.size2['N_IN_CHANNELS'])).astype(np.float32)).cuda()
        bias = torch.from_numpy(
            np.array([0.4181, 0.0201, 0.5681]).reshape(1, 1, 1, N_OUTPUT_CHANNELS).astype(np.float32)).cuda()
        bias_multiplier = torch.from_numpy(
            np.array([
                0.7481, 0.9403, 0.8593, 0.8368, 0.5047, 0.6788, 0.0645, 0.4594, 0.0701, 0.1217, 0.8155, 0.6169, 0.3832,
                0.0896, 0.9372, 0.6916, 0.9577, 0.4232, 0.4020, 0.7394, 0.1302, 0.3343, 0.3463, 0.4212, 0.1637, 0.4978,
                0.3118, 0.9783, 0.7107, 0.8753
            ]).reshape(1, 1, 1, self.size2['HEIGHT'] * self.size2['WIDTH']).astype(np.float32)).cuda()

        expected_output = [
            0.671986, 0.658782, 0.597043, 0.697669, 0.466547, 0.612962, 0.395985, 0.534613, 0.339078, 0.389449,
            0.588685, 0.480602, 0.513149, 0.267177, 0.690284, 0.583441, 0.600374, 0.382231, 0.491456, 0.648562,
            0.39585, 0.507862, 0.451951, 0.527353, 0.413409, 0.498132, 0.472929, 0.747399, 0.549319, 0.721659,
            0.253855, 0.261389, 0.27992, 0.270569, 0.260576, 0.246043, 0.288754, 0.228665, 0.246501, 0.241127,
            0.270341, 0.286458, 0.268663, 0.274518, 0.240328, 0.251526, 0.3071, 0.292441, 0.242923, 0.233312, 0.226182,
            0.27546, 0.225598, 0.271522, 0.243818, 0.239534, 0.236733, 0.210178, 0.285109, 0.255728, 0.424996,
            0.534184, 0.488168, 0.475386, 0.28672, 0.385626, 0.0366424, 0.260985, 0.0398238, 0.0691378, 0.463286,
            0.350461, 0.217696, 0.0509018, 0.532423, 0.392898, 0.544069, 0.24042, 0.228376, 0.420053, 0.0739666,
            0.189916, 0.196733, 0.239284, 0.092998, 0.2828, 0.177134, 0.555772, 0.403749, 0.497258, 0.667269, 0.772161,
            0.779177, 0.743203, 0.576786, 0.549666, 0.450526, 0.545224, 0.424087, 0.505938, 0.677886, 0.618406,
            0.509531, 0.470438, 0.781173, 0.660269, 0.736484, 0.563621, 0.532482, 0.672608, 0.301181, 0.557726,
            0.417402, 0.531035, 0.410525, 0.627554, 0.549296, 0.861714, 0.582198, 0.74991, 0.350651, 0.27908, 0.373943,
            0.321389, 0.256763, 0.344023, 0.219994, 0.302957, 0.318713, 0.208952, 0.290419, 0.418547, 0.239195,
            0.345861, 0.419108, 0.350007, 0.249345, 0.312315, 0.252912, 0.282055, 0.266074, 0.264956, 0.270891,
            0.265288, 0.271062, 0.3551, 0.197069, 0.239888, 0.2531, 0.255761, 0.424996, 0.534184, 0.488168, 0.475386,
            0.28672, 0.385626, 0.0366424, 0.260985, 0.0398238, 0.0691378, 0.463286, 0.350461, 0.217696, 0.0509018,
            0.532423, 0.392898, 0.544069, 0.24042, 0.228376, 0.420053, 0.0739666, 0.189916, 0.196733, 0.239284,
            0.092998, 0.2828, 0.177134, 0.555772, 0.403749, 0.497258
        ]
        actual_output = permutohedral_layer.PermutohedralFunction.apply(NEIGHBORHOOD_SIZE, GROUP, N_OUTPUT_CHANNELS,
                                                                        DO_SKIP_BLUR, weights, bias, bias_multiplier,
                                                                        self.data2.cuda(),
                                                                        self.features2.cuda()).view(-1)
        for i in range(len(expected_output)):
            assert abs(expected_output[i] - actual_output[i].item()) < 1e-6

    def test_backward_with_bias1(self):
        NEIGHBORHOOD_SIZE = 3
        N_OUTPUT_CHANNELS = 2
        GROUP = 2
        DO_SKIP_BLUR = False
        # USE_BIAS_TERM = True

        weights = torch.from_numpy(
            np.array([
                0.9563, 0.9710, 0.1391, 0.7237, 0.0513, 0.9796, 0.5293, 0.8683, 0.9887, 0.3814, 0.5046, 0.0728, 0.6375,
                0.8009, 0.7446, 0.4081, 0.4133, 0.7653, 0.7872, 0.3806, 0.9470, 0.4092, 0.7763, 0.7680, 0.5838, 0.5031,
                0.2360, 0.6857, 0.0998, 0.0508, 0.3512, 0.4108, 0.2213, 0.9582, 0.9080, 0.8887, 0.1852, 0.5220, 0.4328,
                0.5508, 0.1733, 0.1334, 0.0827, 0.5330, 0.1629, 0.1145, 0.4290, 0.6580, 0.9343, 0.4470, 0.0170, 0.6574,
                0.7234, 0.7337, 0.4713, 0.2950, 0.3095, 0.9189, 0.6696, 0.4937, 0.1802, 0.8292, 0.5326, 0.3017, 0.9982,
                0.8356, 0.4158, 0.2481, 0.3167, 0.0646, 0.3026, 0.0480, 0.5483, 0.4559
            ]).reshape(N_OUTPUT_CHANNELS, self.size1['N_DATA_CHANNELS'] // GROUP, 1,
                       get_filter_size(NEIGHBORHOOD_SIZE, self.size1['N_IN_CHANNELS'])).astype(np.float32))
        bias = torch.from_numpy(np.array([0.0818, 0.8894]).reshape(1, 1, 1, N_OUTPUT_CHANNELS).astype(np.float32))
        bias_multiplier = torch.from_numpy(
            np.array([
                0.4784, 0.9353, 0.7067, 0.7665, 0.6245, 0.8526, 0.0751, 0.7218, 0.8618, 0.1907, 0.0649, 0.9583, 0.6774,
                0.1310, 0.1830, 0.2861, 0.7582, 0.1888, 0.3619, 0.5874, 0.5918, 0.9045, 0.4876, 0.7668, 0.9014, 0.7652,
                0.8306, 0.0115, 0.3857, 0.2601
            ]).reshape(1, 1, 1, self.size1['HEIGHT'] * self.size1['WIDTH']).astype(np.float32))

        weights.requires_grad_(True)
        bias.requires_grad_(True)
        self.data1.requires_grad_(True)
        output = permutohedral_layer.PermutohedralFunction.apply(NEIGHBORHOOD_SIZE, GROUP, N_OUTPUT_CHANNELS,
                                                                 DO_SKIP_BLUR, weights.cuda(), bias.cuda(),
                                                                 bias_multiplier.cuda(), self.data1.cuda(),
                                                                 self.features1.cuda()).view(-1)
        output.sum().backward()

        expected_weights_grad = [
            4.02864, 1.9967, 0.00377635, 0, 1.30078, 2.49957, 0.376468, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1.34393, 1.17116,
            0.01726, 0, 0.650069, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -0.649555, -0.771756, -0.0255059, 0,
            0.455789, -0.632493, -0.115808, 0, 0, 0, 0, 0, 0, 0, 0, 0, -0.323614, -0.497247, -0.116576, 0, 0.44479, 0,
            0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0
        ]
        expected_bias_grad = [16.3156, 16.3156]
        expected_data_grad = [
            1.36473, 1.26829, 1.17185, 1.07541, 0.978968, 0.882527, 1.3205, 1.25828, 1.16184, 1.0654, 0.968961,
            0.83203, 1.27626, 1.23911, 1.15184, 1.0554, 0.948105, 0.781534, 1.23202, 1.19487, 1.14183, 1.04539,
            0.897608, 0.731037, 1.18779, 1.15063, 1.11348, 1.01368, 0.847112, 0.680541, 0.491549, 0.526269, 0.560989,
            0.595709, 0.63043, 0.66515, 0.546521, 0.58008, 0.6148, 0.64952, 0.68424, 0.644144, 0.601493, 0.634202,
            0.668611, 0.703331, 0.718004, 0.623138, 0.656465, 0.689174, 0.722421, 0.757142, 0.696998, 0.602132,
            0.711437, 0.744146, 0.776855, 0.770858, 0.675992, 0.581126
        ]

        actual_weights_grad = weights.grad.clone().view(-1)
        actual_bias_grad = bias.grad.clone().view(-1)
        actual_data_grad = self.data1.grad.clone().view(-1)

        for i in range(len(expected_weights_grad)):
            assert abs(expected_weights_grad[i] - actual_weights_grad[i].item()) < 1e-4

        for i in range(len(expected_bias_grad)):
            assert abs(expected_bias_grad[i] - actual_bias_grad[i].item()) < 1e-4

        for i in range(len(expected_data_grad)):
            assert abs(expected_data_grad[i] - actual_data_grad[i].item()) < 1e-4

    def test_backward_with_bias2(self):
        NEIGHBORHOOD_SIZE = 3
        N_OUTPUT_CHANNELS = 3
        GROUP = 2
        DO_SKIP_BLUR = False
        # USE_BIAS_TERM = True

        weights = torch.from_numpy(
            np.array([
                0.56018, 0.66077, 0.47967, 0.93142, 0.77315, 0.21726, 0.33502, 0.40209, 0.26237, 0.53436, 0.96326,
                0.77337, 0.094630, 0.50813, 0.0063933, 0.67345, 0.10891, 0.43981, 0.99030, 0.41358, 0.069789, 0.24911,
                0.69059, 0.30854, 0.027439, 0.82271, 0.33332, 0.098550, 0.84825, 0.79608, 0.43093, 0.68114, 0.17651,
                0.69392, 0.96957, 0.19845, 0.72236, 0.52883, 0.82716, 0.53459, 0.80694, 0.0017458, 0.26308, 0.99736,
                0.63688, 0.044737, 0.64004, 0.34618, 0.039204, 0.75342, 0.22527, 0.40271, 0.86862, 0.23436, 0.00039381,
                0.79662, 0.50522, 0.97183, 0.95994, 0.12843, 0.17314, 0.049051, 0.32262, 0.084470, 0.17357, 0.76326,
                0.019772, 0.49854, 0.21618, 0.14949, 0.28325, 0.77910, 0.15810, 0.17874, 0.75375, 0.43863, 0.42561,
                0.52256, 0.94468, 0.00015473, 0.55581, 0.085251, 0.87456, 0.032248, 0.49668, 0.19325, 0.31288, 0.98346,
                0.93764, 0.17975, 0.55441, 0.28111, 0.22934, 0.56666, 0.90331, 0.95067, 0.76162, 0.92267, 0.21433,
                0.85191, 0.71548, 0.074870, 0.69556, 0.81133, 0.69694, 0.91801, 0.56038, 0.92392, 0.68421, 0.065922,
                0.56490
            ]).reshape(N_OUTPUT_CHANNELS, self.size2['N_DATA_CHANNELS'] // GROUP, 1,
                       get_filter_size(NEIGHBORHOOD_SIZE, self.size2['N_IN_CHANNELS'])).astype(np.float32))
        bias = torch.from_numpy(
            np.array([0.4181, 0.0201, 0.5681]).reshape(1, 1, 1, N_OUTPUT_CHANNELS).astype(np.float32))
        bias_multiplier = torch.from_numpy(
            np.array([
                0.7481, 0.9403, 0.8593, 0.8368, 0.5047, 0.6788, 0.0645, 0.4594, 0.0701, 0.1217, 0.8155, 0.6169, 0.3832,
                0.0896, 0.9372, 0.6916, 0.9577, 0.4232, 0.4020, 0.7394, 0.1302, 0.3343, 0.3463, 0.4212, 0.1637, 0.4978,
                0.3118, 0.9783, 0.7107, 0.8753
            ]).reshape(1, 1, 1, self.size2['HEIGHT'] * self.size2['WIDTH']).astype(np.float32))

        weights.requires_grad_(True)
        bias.requires_grad_(True)
        self.data2.requires_grad_(True)
        output = permutohedral_layer.PermutohedralFunction.apply(NEIGHBORHOOD_SIZE, GROUP, N_OUTPUT_CHANNELS,
                                                                 DO_SKIP_BLUR, weights.cuda(), bias.cuda(),
                                                                 bias_multiplier.cuda(), self.data2.cuda(),
                                                                 self.features2.cuda()).view(-1)
        output.sum().backward()

        expected_weights_grad = [
            14.8956, 4.69098, 0.127646, 0, 5.07123, 4.59785, 0.0131033, 0, 0.0193703, 0.0203993, 0, 0, 0, 0, 0, 0,
            5.17708, 5.48728, 0.419449, 0, 3.87498, 0.239196, 0, 0.0816344, 0.0189134, 0, 0, 0.578986, 0, 0, 0, 0, 0,
            0, 0, 0, 0, 12.8962, 3.74145, 0.106628, 0, 4.07147, 3.25895, 0.0113642, 0, 0.0124502, 0.00997637, 0, 0, 0,
            0, 0, 0, 4.65111, 4.90465, 0.362878, 0, 3.45008, 0.153743, 0, 0.0741479, 0.0162312, 0, 0, 0.536282, 0, 0,
            0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
            0, 0, 0, 0, 0, 0, 0, 0
        ]
        expected_bias_grad = [32.2192, 32.2192, 32.2192]
        expected_data_grad = [
            0.640261, 0.654069, 0.612741, 0.521292, 0.62082, 0.500441, 0.443001, 0.696862, 0.68263, 0.563524, 0.709144,
            0.607451, 0.505248, 0.614892, 0.642533, 0.656084, 0.631512, 0.650687, 0.68876, 0.692149, 0.699397,
            0.523488, 0.650926, 0.474139, 0.578441, 0.640998, 0.62385, 0.786949, 0.640746, 0.63119, 0.536208, 0.600437,
            0.579162, 0.63255, 0.571779, 0.549215, 0.687047, 0.497596, 0.510347, 0.602985, 0.673712, 0.583834,
            0.643347, 0.576484, 0.551837, 0.537292, 0.628595, 0.643528, 0.504792, 0.502374, 0.49481, 0.624513,
            0.543622, 0.638546, 0.589109, 0.552862, 0.554695, 0.695636, 0.550802, 0.544424, 0, 0, 0, 0, 0, 0, 0, 0, 0,
            0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.686512, 0.640956, 0.470254, 0.561622,
            0.650877, 0.649095, 0.702604, 0.637288, 0.503675, 0.704234, 0.60799, 0.761998, 0.660147, 0.493831, 0.72747,
            0.691375, 0.656526, 0.519583, 0.652258, 0.626416, 0.590745, 0.635131, 0.598246, 0.635316, 0.617589,
            0.492191, 0.725176, 0.682799, 0.629714, 0.66715, 0.635869, 0.551671, 0.644433, 0.597978, 0.545896,
            0.639368, 0.51812, 0.589414, 0.575996, 0.518513, 0.56925, 0.705425, 0.540455, 0.638895, 0.647447, 0.631445,
            0.542258, 0.55456, 0.545102, 0.559479, 0.577441, 0.556773, 0.573644, 0.554368, 0.564004, 0.639277,
            0.505371, 0.53077, 0.556392, 0.537153, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
            0, 0, 0, 0, 0, 0, 0
        ]

        actual_weights_grad = weights.grad.clone().view(-1)
        actual_bias_grad = bias.grad.clone().view(-1)
        actual_data_grad = self.data2.grad.clone().view(-1)

        for i in range(len(expected_weights_grad)):
            assert abs(expected_weights_grad[i] - actual_weights_grad[i].item()) < 1e-4

        for i in range(len(expected_bias_grad)):
            assert abs(expected_bias_grad[i] - actual_bias_grad[i].item()) < 1e-4

        for i in range(len(expected_data_grad)):
            assert abs(expected_data_grad[i] - actual_data_grad[i].item()) < 1e-4

    def test_backward_skip_blur(self):
        NEIGHBORHOOD_SIZE = 0
        N_OUTPUT_CHANNELS = self.size2['N_DATA_CHANNELS']
        GROUP = 2
        DO_SKIP_BLUR = True
        # USE_BIAS_TERM = True

        weights = torch.from_numpy(
            np.array([0.7361, 0.4956,
                      0.2333]).reshape(N_OUTPUT_CHANNELS, self.size2['N_DATA_CHANNELS'] // GROUP, 1,
                                       get_filter_size(NEIGHBORHOOD_SIZE,
                                                       self.size2['N_IN_CHANNELS'])).astype(np.float32))
        bias = torch.from_numpy(
            np.array([0.9585, 0.7999, 0.6787]).reshape(1, 1, 1, N_OUTPUT_CHANNELS).astype(np.float32))
        bias_multiplier = torch.from_numpy(
            np.array([
                0.7481, 0.9403, 0.8593, 0.8368, 0.5047, 0.6788, 0.0645, 0.4594, 0.0701, 0.1217, 0.8155, 0.6169, 0.3832,
                0.0896, 0.9372, 0.6916, 0.9577, 0.4232, 0.4020, 0.7394, 0.1302, 0.3343, 0.3463, 0.4212, 0.1637, 0.4978,
                0.3118, 0.9783, 0.7107, 0.8753
            ]).reshape(1, 1, 1, self.size2['HEIGHT'] * self.size2['WIDTH']).astype(np.float32))

        weights.requires_grad_(True)
        bias.requires_grad_(True)
        self.data2.requires_grad_(True)
        expected_output = [
            1.2106397, 1.4059553, 1.325345, 1.3064574, 0.98810768, 1.1588342, 0.56082177, 0.93969607, 0.55695707,
            0.62310159, 1.2788339, 1.0897236, 0.87027693, 0.58292449, 1.4076798, 1.1645176, 1.4104064, 0.8977896,
            0.87913656, 1.2097256, 0.62138504, 0.81554753, 0.84083241, 0.90731347, 0.66045427, 0.98448342, 0.80185634,
            1.4451666, 1.1708717, 1.335151, 1.0506576, 1.1768298, 1.1207587, 1.0797988, 0.83171844, 0.95271587,
            0.45668876, 0.81610072, 0.52772063, 0.51030838, 1.0840322, 0.93393165, 0.71674114, 0.51630175, 1.1676031,
            0.9915688, 1.2142869, 0.78521383, 0.78360116, 1.0352564, 0.56032115, 0.69389021, 0.69681281, 0.74348116,
            0.5516637, 0.82095051, 0.67815298, 1.1907494, 1.034319, 1.1449763, 1.0893025, 1.2351354, 1.1800677,
            1.1698532, 0.93925953, 1.0618761, 0.65183693, 0.89274019, 0.62371844, 0.6818673, 1.1506127, 1.0151422,
            0.86286885, 0.65527904, 1.2322788, 1.0589175, 1.2474821, 0.8846873, 0.8508262, 1.0849214, 0.66654968,
            0.82367837, 0.82985705, 0.89085257, 0.70706683, 0.93311822, 0.80288106, 1.2605779, 1.0677499, 1.1788977,
            1.3125129, 1.4878433, 1.4122241, 1.3919618, 1.0693264, 1.2814786, 0.62099034, 1.0365723, 0.65727925,
            0.6467033, 1.3823378, 1.1840477, 0.95180392, 0.6669209, 1.4913067, 1.2544708, 1.506516, 0.99617732,
            0.97062719, 1.3008759, 0.76374, 0.89158124, 0.95580459, 0.99541795, 0.75488198, 1.064127, 0.83581799,
            1.474679, 1.2969515, 1.416678, 1.1608446, 1.2968783, 1.2612717, 1.2418523, 0.90263855, 1.0108664,
            0.52185768, 0.92621124, 0.6298871, 0.47594017, 1.1978368, 1.0663919, 0.72849476, 0.62213624, 1.3228945,
            1.1260641, 1.1579833, 0.91223216, 0.81364024, 1.1405958, 0.50637233, 0.77872044, 0.72504294, 0.85455918,
            0.66576171, 0.96620578, 0.58374196, 1.1844332, 0.90162659, 1.1918765, 1.1160285, 1.2337053, 1.1916478,
            1.1781456, 0.91453326, 1.0383097, 0.5886054, 0.91890091, 0.65751314, 0.56648892, 1.1561997, 1.03036,
            0.79282928, 0.65363938, 1.2479124, 1.0810653, 1.1695704, 0.89767015, 0.84125835, 1.1022732, 0.63740772,
            0.79745662, 0.79987085, 0.87023067, 0.7071346, 0.94290167, 0.67735487, 1.16265, 0.98544693, 1.1586425
        ]
        actual_output = permutohedral_layer.PermutohedralFunction.apply(NEIGHBORHOOD_SIZE, GROUP, N_OUTPUT_CHANNELS,
                                                                        DO_SKIP_BLUR, weights.cuda(), bias.cuda(),
                                                                        bias_multiplier.cuda(), self.data2.cuda(),
                                                                        self.features2.cuda()).view(-1)
        for i in range(len(expected_output)):
            assert abs(expected_output[i] - actual_output[i].item()) < 1e-6

        actual_output.sum().backward()

        expected_weights_grad = [0, 0, 0]
        expected_bias_grad = [32.2192, 32.2192, 32.2192]
        expected_data_grad = [
            0.878787, 1.0793, 1.05958, 1.06437, 1.13338, 1.02147, 0.890642, 1.02077, 0.846777, 1.15252, 0.790584,
            0.983239, 1.01643, 0.956648, 1.3147, 1.06643, 0.81699, 0.78568, 0.91075, 1.05869, 0.963086, 0.852969,
            1.29773, 0.978801, 1.06955, 1.23564, 1.07711, 0.916518, 0.839022, 0.921841, 0.878787, 1.0793, 1.05958,
            1.06437, 1.13338, 1.02147, 0.890642, 1.02077, 0.846777, 1.15252, 0.790584, 0.983239, 1.01643, 0.956648,
            1.3147, 1.06643, 0.81699, 0.78568, 0.91075, 1.05869, 0.963086, 0.852969, 1.29773, 0.978801, 1.06955,
            1.23564, 1.07711, 0.916518, 0.839022, 0.921841, 0.878787, 1.0793, 1.05958, 1.06437, 1.13338, 1.02147,
            0.890642, 1.02077, 0.846777, 1.15252, 0.790584, 0.983239, 1.01643, 0.956648, 1.3147, 1.06643, 0.81699,
            0.78568, 0.91075, 1.05869, 0.963086, 0.852969, 1.29773, 0.978801, 1.06955, 1.23564, 1.07711, 0.916518,
            0.839022, 0.921841, 1.07815, 1.24678, 0.979744, 1.22129, 1.06917, 0.768681, 1.04066, 1.20944, 1.05833,
            0.835302, 1.2032, 0.894564, 0.885976, 0.919324, 0.77283, 1.12701, 0.831053, 1.09817, 1.04856, 1.24888,
            0.78603, 1.06022, 0.875274, 1.11727, 1.16575, 1.00594, 0.811235, 0.84492, 0.731213, 1.06503, 1.07815,
            1.24678, 0.979744, 1.22129, 1.06917, 0.768681, 1.04066, 1.20944, 1.05833, 0.835302, 1.2032, 0.894564,
            0.885976, 0.919324, 0.77283, 1.12701, 0.831053, 1.09817, 1.04856, 1.24888, 0.78603, 1.06022, 0.875274,
            1.11727, 1.16575, 1.00594, 0.811235, 0.84492, 0.731213, 1.06503, 1.07815, 1.24678, 0.979744, 1.22129,
            1.06917, 0.768681, 1.04066, 1.20944, 1.05833, 0.835302, 1.2032, 0.894564, 0.885976, 0.919324, 0.77283,
            1.12701, 0.831053, 1.09817, 1.04856, 1.24888, 0.78603, 1.06022, 0.875274, 1.11727, 1.16575, 1.00594,
            0.811235, 0.84492, 0.731213, 1.06503
        ]
        actual_weights_grad = weights.grad.clone().view(-1)
        actual_bias_grad = bias.grad.clone().view(-1)
        actual_data_grad = self.data2.grad.clone().view(-1)

        for i in range(len(expected_weights_grad)):
            assert abs(expected_weights_grad[i] - actual_weights_grad[i].item()) < 1e-4

        for i in range(len(expected_bias_grad)):
            assert abs(expected_bias_grad[i] - actual_bias_grad[i].item()) < 1e-4

        for i in range(len(expected_data_grad)):
            assert abs(expected_data_grad[i] - actual_data_grad[i].item()) < 1e-4


if __name__ == '__main__':
    unittest.main()
